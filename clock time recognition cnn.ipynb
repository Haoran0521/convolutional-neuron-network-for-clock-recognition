{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 64, 64, 1])\n",
    "    \n",
    "    # First convolutional layer\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    # First Pooling layer\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    # Second Pooling layer\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2], strides=2)\n",
    "    \n",
    "    # Flat layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 16 * 16 * 64])\n",
    "    \n",
    "    # Dense layer\n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=256, activation=tf.nn.relu)\n",
    "    \n",
    "    \n",
    "    # dropping out operation\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense1, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "    predictions = {\n",
    "        # new try\n",
    "        \"hour\": tf.argmax(input = logits[0], axis=1),\n",
    "        \"minute\": tf.argmax(input = logits[1], axis=1)\n",
    "#       # Generate predictions (for PREDICT and EVAL mode)\n",
    "#       \"classes\": tf.argmax(input=logits, axis=1),\n",
    "#       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "#       # `logging_hook`.\n",
    "#       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.mean_squared_error(labels=labels,predictions=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions)}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = x.reshape(x.shape[0], 64, 64, 1)\n",
    "    share = 400\n",
    "    x_train = x[:share]\n",
    "    y_train = y[:share]\n",
    "    x_val = x[share:]\n",
    "    y_val = y[share:]\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0507 01:28:40.407777 47154798759616 estimator.py:1778] Using default config.\n",
      "W0507 01:28:40.409107 47154798759616 estimator.py:1799] Using temporary folder as model directory: /local_scratch/8072374/tmpprjhfjke\n",
      "I0507 01:28:40.410302 47154798759616 estimator.py:202] Using config: {'_model_dir': '/local_scratch/8072374/tmpprjhfjke', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x2ae3528ccdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0507 01:28:40.439139 47154798759616 estimator.py:1126] Calling model_fn.\n",
      "I0507 01:28:40.677817 47154798759616 estimator.py:1128] Done calling model_fn.\n",
      "I0507 01:28:40.680263 47154798759616 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0507 01:28:41.092745 47154798759616 monitored_session.py:241] Graph was finalized.\n",
      "I0507 01:28:42.033881 47154798759616 session_manager.py:500] Running local_init_op.\n",
      "I0507 01:28:42.039641 47154798759616 session_manager.py:502] Done running local_init_op.\n",
      "W0507 01:28:42.052176 47154798759616 deprecation.py:323] From /software/tensorflow/2.0.0a/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:877: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0507 01:28:42.384108 47154798759616 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /local_scratch/8072374/tmpprjhfjke/model.ckpt.\n",
      "I0507 01:28:44.509377 47154798759616 basic_session_run_hooks.py:249] loss = 560.6349, step = 1\n",
      "I0507 01:29:01.017763 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.05751\n",
      "I0507 01:29:01.020451 47154798759616 basic_session_run_hooks.py:247] loss = 484.10547, step = 101 (16.511 sec)\n",
      "I0507 01:29:17.774191 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 5.96778\n",
      "I0507 01:29:17.776762 47154798759616 basic_session_run_hooks.py:247] loss = 263.142, step = 201 (16.756 sec)\n",
      "I0507 01:29:34.229303 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.07715\n",
      "I0507 01:29:34.231878 47154798759616 basic_session_run_hooks.py:247] loss = 122.71389, step = 301 (16.455 sec)\n",
      "I0507 01:29:50.678103 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.07941\n",
      "I0507 01:29:50.679868 47154798759616 basic_session_run_hooks.py:247] loss = 136.24081, step = 401 (16.448 sec)\n",
      "I0507 01:30:07.087092 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.09426\n",
      "I0507 01:30:07.089515 47154798759616 basic_session_run_hooks.py:247] loss = 259.16016, step = 501 (16.410 sec)\n",
      "I0507 01:30:23.516815 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.08648\n",
      "I0507 01:30:23.518603 47154798759616 basic_session_run_hooks.py:247] loss = 223.17123, step = 601 (16.429 sec)\n",
      "I0507 01:30:39.890354 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.10742\n",
      "I0507 01:30:39.892206 47154798759616 basic_session_run_hooks.py:247] loss = 207.25107, step = 701 (16.374 sec)\n",
      "I0507 01:30:56.308852 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.09074\n",
      "I0507 01:30:56.311203 47154798759616 basic_session_run_hooks.py:247] loss = 233.60555, step = 801 (16.419 sec)\n",
      "I0507 01:31:12.737467 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.08688\n",
      "I0507 01:31:12.738911 47154798759616 basic_session_run_hooks.py:247] loss = 127.6689, step = 901 (16.428 sec)\n",
      "I0507 01:31:29.131405 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.09982\n",
      "I0507 01:31:29.133090 47154798759616 basic_session_run_hooks.py:247] loss = 143.84816, step = 1001 (16.394 sec)\n",
      "I0507 01:31:45.626394 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.06246\n",
      "I0507 01:31:45.628200 47154798759616 basic_session_run_hooks.py:247] loss = 183.97495, step = 1101 (16.495 sec)\n",
      "I0507 01:32:02.075267 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.07948\n",
      "I0507 01:32:02.077615 47154798759616 basic_session_run_hooks.py:247] loss = 152.12921, step = 1201 (16.449 sec)\n",
      "I0507 01:32:18.544520 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.07187\n",
      "I0507 01:32:18.546211 47154798759616 basic_session_run_hooks.py:247] loss = 184.06134, step = 1301 (16.469 sec)\n",
      "I0507 01:32:34.998973 47154798759616 basic_session_run_hooks.py:680] global_step/sec: 6.07739\n",
      "I0507 01:32:35.000681 47154798759616 basic_session_run_hooks.py:247] loss = 112.70171, step = 1401 (16.454 sec)\n",
      "I0507 01:32:51.860429 47154798759616 basic_session_run_hooks.py:594] Saving checkpoints for 1500 into /local_scratch/8072374/tmpprjhfjke/model.ckpt.\n",
      "I0507 01:32:51.962781 47154798759616 estimator.py:360] Loss for final step: 115.73769.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-645d4e890c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0;31m# 不能用cross entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m# 2 output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/tensorflow/2.0.0a/lib/python3.7/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/software/tensorflow/2.0.0a/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/tensorflow/2.0.0a/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-645d4e890c1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m     32\u001b[0m   eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n\u001b[1;32m     33\u001b[0m       x={\"x\": eval_data}, y=eval_labels, num_epochs=1, shuffle=False)\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "  # Load training and eval data\n",
    "  image = np.load(\"clock_image.npy\") \n",
    "  label = np.load(\"clock_time.npy\")\n",
    "  image  = tf.keras.utils.normalize(image, axis=1)  # scales data between 0 and 1\n",
    "\n",
    "  train_data, train_labels, eval_data, eval_labels = preprocess(image, label)\n",
    "\n",
    "  # Create the Estimator\n",
    "  classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=None)\n",
    "\n",
    "#   # Set up logging for predictions\n",
    "#   # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "#   tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "#   logging_hook = tf.train.LoggingTensorHook(\n",
    "#       tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "  # Train the model\n",
    "  train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=16,\n",
    "      num_epochs=60,\n",
    "      shuffle=True)\n",
    "  classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=20000)\n",
    "#       hooks=[logging_hook])\n",
    "\n",
    "  # Evaluate the model and print results\n",
    "  eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data}, y=eval_labels, num_epochs=1, shuffle=False)\n",
    "  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "  print(eval_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n",
    "  # 不能用cross entropy\n",
    "  # 2 output layer\n",
    "  # 做成regression -》 不能用cross entropy，用MSE\n",
    "  # 更了解linear和logistic regression\n",
    "\n",
    "  # 不要probability，改loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow 2.0.0 alpha GPU)",
   "language": "python",
   "name": "tensorflow-2.0.0a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
